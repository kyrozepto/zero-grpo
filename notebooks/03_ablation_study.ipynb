{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 03 - Component Ablation Study\n",
                "\n",
                "Objective: Quantify the contribution of each reward component and validate correlation with human judgment.\n",
                "\n",
                "## Expected Correlations (with human scores)\n",
                "- R_warp ↔ Human: ρ ~0.6-0.7\n",
                "- R_epi ↔ Human: ρ ~0.5-0.6\n",
                "- R_sem ↔ Human: ρ ~0.4-0.5\n",
                "- R_total ↔ Human: ρ ~0.7-0.8 (best)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.append('..')\n",
                "\n",
                "import torch\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from scipy import stats\n",
                "from tqdm import tqdm\n",
                "\n",
                "from src.rewards import CompositeReward\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Using device: {device}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "reward_fn = CompositeReward(device=device)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Generate Samples for Human Annotation\n",
                "\n",
                "We create 100 diverse samples for human scoring."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_annotatable_samples(n_samples=100, seed=42):\n",
                "    \"\"\"Generate samples with known consistency levels for annotation.\"\"\"\n",
                "    torch.manual_seed(seed)\n",
                "    \n",
                "    samples = []\n",
                "    conditions = []\n",
                "    ground_truth = []  # Simulated human scores (1-5)\n",
                "    \n",
                "    for i in range(n_samples):\n",
                "        H, W = 320, 320\n",
                "        base = torch.randn(3, H, W) * 0.2 + 0.5\n",
                "        base = torch.clamp(base, 0, 1)\n",
                "        \n",
                "        # Create samples with known quality levels\n",
                "        consistency = np.linspace(0.2, 1.0, n_samples)[i]\n",
                "        \n",
                "        views = []\n",
                "        for v in range(6):\n",
                "            noise = torch.randn(3, H, W) * 0.1 * (1 - consistency)\n",
                "            view = torch.clamp(base + noise, 0, 1)\n",
                "            views.append(view)\n",
                "        \n",
                "        views = torch.stack(views)\n",
                "        samples.append(views)\n",
                "        conditions.append(base)\n",
                "        \n",
                "        # Simulated human score (correlated with consistency)\n",
                "        human_score = 1 + 4 * consistency + np.random.normal(0, 0.3)\n",
                "        human_score = np.clip(human_score, 1, 5)\n",
                "        ground_truth.append(human_score)\n",
                "    \n",
                "    return torch.stack(samples), torch.stack(conditions), np.array(ground_truth)\n",
                "\n",
                "samples, conditions, human_scores = generate_annotatable_samples(100)\n",
                "print(f\"Generated {len(samples)} samples for ablation study\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Compute All Reward Variants"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results = []\n",
                "\n",
                "for i in tqdm(range(len(samples)), desc=\"Computing rewards\"):\n",
                "    sample = samples[i:i+1].to(device)\n",
                "    condition = conditions[i:i+1].to(device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        result = reward_fn.forward(sample, condition, step=5000)\n",
                "    \n",
                "    results.append({\n",
                "        'sample_id': i,\n",
                "        'r_warp': result['warp'].cpu().item(),\n",
                "        'r_epi': result['epipolar'].cpu().item(),\n",
                "        'r_sem': result['semantic'].cpu().item(),\n",
                "        'r_geom': result['geometric'].cpu().item(),\n",
                "        'r_total': result['total'].cpu().item(),\n",
                "        'human_score': human_scores[i]\n",
                "    })\n",
                "\n",
                "df = pd.DataFrame(results)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Correlation Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "reward_cols = ['r_warp', 'r_epi', 'r_sem', 'r_geom', 'r_total']\n",
                "correlations = {}\n",
                "\n",
                "print(\"\\nSpearman Correlations with Human Scores:\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "for col in reward_cols:\n",
                "    rho, p_value = stats.spearmanr(df[col], df['human_score'])\n",
                "    correlations[col] = {'rho': rho, 'p_value': p_value}\n",
                "    status = '✅' if rho > 0.5 else '⚠️'\n",
                "    print(f\"{col:10s}: ρ = {rho:.4f}, p = {p_value:.6f} {status}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization: Reward vs Human Score\n",
                "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
                "\n",
                "for ax, col in zip(axes.flat[:-1], reward_cols):\n",
                "    ax.scatter(df[col], df['human_score'], alpha=0.6, s=30)\n",
                "    \n",
                "    # Fit line\n",
                "    z = np.polyfit(df[col], df['human_score'], 1)\n",
                "    p = np.poly1d(z)\n",
                "    x_line = np.linspace(df[col].min(), df[col].max(), 100)\n",
                "    ax.plot(x_line, p(x_line), 'r--', alpha=0.8)\n",
                "    \n",
                "    rho = correlations[col]['rho']\n",
                "    ax.set_xlabel(col)\n",
                "    ax.set_ylabel('Human Score')\n",
                "    ax.set_title(f'{col} (ρ = {rho:.3f})')\n",
                "    ax.grid(True, alpha=0.3)\n",
                "\n",
                "# Hide unused subplot\n",
                "axes.flat[-1].axis('off')\n",
                "\n",
                "plt.suptitle('Reward Components vs Human Judgment', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.savefig('../results/ablation_correlations.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Ablation: Removing Components"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compute ablated rewards\n",
                "df['r_no_warp'] = df['r_epi'] * df['r_sem']  # Without depth warping\n",
                "df['r_no_epi'] = df['r_warp'] * df['r_sem']  # Without epipolar\n",
                "df['r_no_sem'] = df['r_geom']  # Without semantic (pure geometric)\n",
                "\n",
                "ablation_cols = ['r_total', 'r_no_warp', 'r_no_epi', 'r_no_sem']\n",
                "\n",
                "print(\"\\nAblation Study - Correlation with Human Scores:\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "ablation_results = []\n",
                "for col in ablation_cols:\n",
                "    rho, p_value = stats.spearmanr(df[col], df['human_score'])\n",
                "    print(f\"{col:15s}: ρ = {rho:.4f}\")\n",
                "    ablation_results.append({'variant': col, 'correlation': rho})\n",
                "\n",
                "# Bar chart\n",
                "plt.figure(figsize=(10, 5))\n",
                "ablation_df = pd.DataFrame(ablation_results)\n",
                "colors = ['green' if x == 'r_total' else 'coral' for x in ablation_df['variant']]\n",
                "plt.bar(ablation_df['variant'], ablation_df['correlation'], color=colors, edgecolor='black')\n",
                "plt.ylabel('Spearman ρ with Human Score')\n",
                "plt.title('Ablation Study: Impact of Removing Reward Components')\n",
                "plt.axhline(y=0.65, color='red', linestyle='--', label='Target ρ > 0.65')\n",
                "plt.legend()\n",
                "plt.tight_layout()\n",
                "plt.savefig('../results/ablation_bar_chart.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Top and Bottom Samples"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Top 5 by each component\n",
                "print(\"\\nTop 5 samples per reward component:\")\n",
                "for col in ['r_warp', 'r_epi', 'r_sem', 'r_total']:\n",
                "    print(f\"\\n{col.upper()}:\")\n",
                "    top5 = df.nlargest(5, col)[['sample_id', col, 'human_score']]\n",
                "    display(top5)\n",
                "\n",
                "print(\"\\nBottom 5 samples per reward component:\")\n",
                "for col in ['r_warp', 'r_epi', 'r_sem', 'r_total']:\n",
                "    print(f\"\\n{col.upper()}:\")\n",
                "    bottom5 = df.nsmallest(5, col)[['sample_id', col, 'human_score']]\n",
                "    display(bottom5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Summary Table"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "summary = []\n",
                "for col in reward_cols:\n",
                "    rho, p = stats.spearmanr(df[col], df['human_score'])\n",
                "    summary.append({\n",
                "        'Component': col,\n",
                "        'Spearman ρ': f'{rho:.4f}',\n",
                "        'p-value': f'{p:.6f}',\n",
                "        'Mean': f'{df[col].mean():.4f}',\n",
                "        'Std': f'{df[col].std():.4f}',\n",
                "        'Meets Target': '✅' if rho > 0.5 else '❌'\n",
                "    })\n",
                "\n",
                "summary_df = pd.DataFrame(summary)\n",
                "display(summary_df)\n",
                "\n",
                "# Save\n",
                "df.to_csv('../results/ablation_study_results.csv', index=False)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}